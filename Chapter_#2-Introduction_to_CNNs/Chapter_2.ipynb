{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b58a8a-5b61-4431-a6db-3b97782929f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T00:39:18.816235Z",
     "iopub.status.busy": "2023-01-01T00:39:18.815224Z",
     "iopub.status.idle": "2023-01-01T00:39:38.722379Z",
     "shell.execute_reply": "2023-01-01T00:39:38.721399Z",
     "shell.execute_reply.started": "2023-01-01T00:39:18.816204Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download dependcies\n",
    "%pip install -q jmd_imagescraper\n",
    "%pip install tensorflow_datasets==4.7\n",
    "%pip install os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03995a44-ab49-4d77-bc29-5a92c138591c",
   "metadata": {},
   "source": [
    "If the imports below does not work, you may need to restart the kernel. This can be done in top right \"RESTART KERNEL\" button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec097d58-c50b-4fe8-8c6b-ca0215b335da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T00:39:38.727739Z",
     "iopub.status.busy": "2023-01-01T00:39:38.727462Z",
     "iopub.status.idle": "2023-01-01T00:39:46.408081Z",
     "shell.execute_reply": "2023-01-01T00:39:46.407184Z",
     "shell.execute_reply.started": "2023-01-01T00:39:38.727715Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import Sequential, losses, metrics, layers\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ba3c4-4ef9-41a8-a860-ded92df9a2d8",
   "metadata": {},
   "source": [
    "The goal in this chapter is to create our own classification model with images sourced from the web. In order to do that, \n",
    "here are the necessary steps: \n",
    "1. Get images for the web\n",
    "2. Process images\n",
    "3. Create dataset with features and labels\n",
    "4. Create model\n",
    "5. Train model\n",
    "6. Evaluate model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d624ef7-cd27-4198-8145-96c9ffc7d16d",
   "metadata": {},
   "source": [
    "1. Get images from the web\n",
    "\n",
    "Imagine you were to go on google, search for an image, and then copy and paste that image to form your dataset. That's what \n",
    "we're doing here, but with code! We're using a module called jmd_imagescraper (which we downloaded and imported in the first \n",
    "two cells). This module usees duckduckgo to search for images and download them to our computer. Don't worry about the first\n",
    "few lines of code in the block below. They're not important to understanding how to train the model. \n",
    "\n",
    "In the block below, we downloaded a bunch of images to our computer (see in the project files, you may need to click the file \n",
    "refresh button for it to appear) and stored the file names inside of a list called cat_images and dog_images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef1cfad-534b-47b4-9bdd-db39cb5ea27d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T00:39:46.456224Z",
     "iopub.status.busy": "2023-01-01T00:39:46.411326Z",
     "iopub.status.idle": "2023-01-01T00:39:59.334047Z",
     "shell.execute_reply": "2023-01-01T00:39:59.331318Z",
     "shell.execute_reply.started": "2023-01-01T00:39:46.456150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duckduckgo search: cute kittens\n",
      "Downloading results into /notebooks/images/Cats\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='100' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [100/100 00:05&lt;00:00 Images downloaded]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duckduckgo search: cute puppies\n",
      "Downloading results into /notebooks/images/Dogs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='100' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [100/100 00:04&lt;00:00 Images downloaded]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "root = Path().cwd()/\"images\"\n",
    "\n",
    "from jmd_imagescraper.core import * # dont't worry, it's designed to work with import *\n",
    "\n",
    "cat_images = duckduckgo_search(root, \"Cats\", \"cute kittens\", max_results=100)\n",
    "dog_images = duckduckgo_search(root, \"Dogs\", \"cute puppies\", max_results=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f51730b5-7627-4bfb-befd-7a0442f8505a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T00:42:18.398672Z",
     "iopub.status.busy": "2023-01-01T00:42:18.398205Z",
     "iopub.status.idle": "2023-01-01T00:42:18.403651Z",
     "shell.execute_reply": "2023-01-01T00:42:18.402743Z",
     "shell.execute_reply.started": "2023-01-01T00:42:18.398644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of cat images:  100\n",
      "amount of dog images:  100\n",
      "\n",
      "first 5 cat image file names:\n",
      "[PosixPath('/notebooks/images/Cats/511_5b8babd3.jpg'), PosixPath('/notebooks/images/Cats/512_11ce4387.jpg'), PosixPath('/notebooks/images/Cats/513_e1d7586a.jpg'), PosixPath('/notebooks/images/Cats/514_867fc83f.jpg'), PosixPath('/notebooks/images/Cats/515_611afe9f.jpg')]\n",
      "\n",
      "irst 5 dog image file names:\n",
      "[PosixPath('/notebooks/images/Dogs/511_86a8570f.jpg'), PosixPath('/notebooks/images/Dogs/512_da65c7b5.jpg'), PosixPath('/notebooks/images/Dogs/513_bbdfd40e.jpg'), PosixPath('/notebooks/images/Dogs/514_ae8a3b09.jpg'), PosixPath('/notebooks/images/Dogs/515_5b159bda.jpg')]\n"
     ]
    }
   ],
   "source": [
    "print('amount of cat images: ', len(cat_images))\n",
    "print('amount of dog images: ', len(cat_images))\n",
    "\n",
    "print('\\nfirst 5 cat image file names:')\n",
    "print(cat_images[:5])\n",
    "\n",
    "print('\\nirst 5 dog image file names:')\n",
    "print(dog_images[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76ed5d-0e76-4cd2-a46d-9340430bcf73",
   "metadata": {},
   "source": [
    "In the codeblock above, we take a look at cat_images and dog_images. They both contain the file paths to where the images are \n",
    "stored in our project. \n",
    "\n",
    "\n",
    "Task: \n",
    "Can you figure out when there are 100 images? How about trying to change this number? Experiment with getting differnt amount of \n",
    "images, and different types of images. Try to get a bunch of different animals!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a885ab-5deb-43aa-96cc-31c16d938e87",
   "metadata": {},
   "source": [
    "2. Process Images\n",
    "\n",
    "Now that we have the images downloaded to our project, we have to prepare them so we can put them in a dataset. Since we're using \n",
    "supervised learning, our dataset needs to have both features and labels. Our features will be the images we use (in this case, a \n",
    "dog image or a cat image) and our labels will be a 0 or a 1. If the image is of a cat, the label will be 0. If the image is of a \n",
    "dog, the label will be a 1. If you were to add an additional class, its label would be 2. \n",
    "\n",
    "To get both the features and the labels, we created the following function to extract both from the image's file path. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3383ef2f-7b7c-4a32-be2a-358d5ecff022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T00:52:29.637925Z",
     "iopub.status.busy": "2023-01-01T00:52:29.636850Z",
     "iopub.status.idle": "2023-01-01T00:52:29.643496Z",
     "shell.execute_reply": "2023-01-01T00:52:29.642598Z",
     "shell.execute_reply.started": "2023-01-01T00:52:29.637887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/images/Cats/002_c0d2c2be.jpg\n",
      "['', 'notebooks', 'images', 'Cats', '002_c0d2c2be.jpg']\n",
      "Cats\n"
     ]
    }
   ],
   "source": [
    "normpath = os.path.normpath('/notebooks/images/Cats/002_c0d2c2be.jpg')\n",
    "split = normpath.split('/')\n",
    "\n",
    "print(normpath)\n",
    "print(split)\n",
    "print(split[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d33d5-749b-4da5-99e5-a61c6dbb5cbd",
   "metadata": {},
   "source": [
    "In get_label, we take a file path (example is /notebooks/images/Cats/002_c0d2c2be.jpg), split it into a list containing\n",
    "[notebooks, images, Cats, 002_c0d2c2be.jpg], and accessed the 2nd to last element where 'Cats' is. This 2nd to last element will \n",
    "always be the folder containing the images of the different classes (in this case Cats or Dogs, if you were to do more it could\n",
    "be Birds or Tiger).\n",
    "\n",
    "In process_file, we load the image from the file path (before we had the location of the image in the computer's memory, now we \n",
    "have the actual image), we then resize the image to 224 pixels by 224 pixels (this helps with model performance, CNNs struggle \n",
    "with images that are too big). The images are then converted to numpy arrays (this is for easier data conversion) and then \n",
    "normalized between 0 and 1. This is a common process in machine learning. Models perform better when the data is scaled to the\n",
    "interval [0,1] or [-1,1]. In RGB images, the range of values is [0,255] when this is divided by 255, the new range is [0, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ee3701-9477-4636-be96-577ab8345e37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T05:06:32.717020Z",
     "iopub.status.busy": "2022-12-31T05:06:32.716543Z",
     "iopub.status.idle": "2022-12-31T05:06:32.724118Z",
     "shell.execute_reply": "2022-12-31T05:06:32.723372Z",
     "shell.execute_reply.started": "2022-12-31T05:06:32.716995Z"
    }
   },
   "outputs": [],
   "source": [
    "CLASSES = ['Cats', 'Dogs'] # same name as the folder which hold the images\n",
    "\n",
    "def get_label(file_path):\n",
    "    label = os.path.normpath(file_path).split(os.path.sep)[-2] # extract the class from the label\n",
    "    return CLASSES.index(label) # return the label \n",
    "\n",
    "def process_file(file_path, img_size=224):\n",
    "    img = Image.open(file_path) # load the image from the url\n",
    "    img = img.resize((img_size, img_size), Image.Resampling.BILINEAR) # resize the image to 224 x 224 pixels\n",
    "    img = np.asarray(img) # convert Image to np array\n",
    "    img = img/255.0 # scale image between 0 and 1 to improve model performance\n",
    "    return img # return the image \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab2b6a-f6aa-45f5-93af-184ff74b82db",
   "metadata": {},
   "source": [
    "3. Create the datast\n",
    "\n",
    "Now that we have the functionality to turn our image_urls to features and labels, its time to start constructed the dataset. \n",
    "The function below takes in a list which holds multiple classes of image urls. It will add all of these classes to the overall\n",
    "training/testing features and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6225f5-3251-4170-84dc-0dc709ddd7c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T05:06:32.725345Z",
     "iopub.status.busy": "2022-12-31T05:06:32.725135Z",
     "iopub.status.idle": "2022-12-31T05:06:32.729743Z",
     "shell.execute_reply": "2022-12-31T05:06:32.729080Z",
     "shell.execute_reply.started": "2022-12-31T05:06:32.725327Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(datasets, training_proportion=0.8): # takes in list of image_urls ex. [cat_images, dog_images]\n",
    "    \n",
    "    training_features, training_labels = [], []\n",
    "    testing_features, testing_labels = [], []\n",
    "    \n",
    "    for image_urls in datasets: # extract image_urls of a single class ex. cat_images\n",
    "        for index, url in enumerate(image_urls): # loop through every url in cat_images\n",
    "\n",
    "            if index < (len(image_urls) * training_proportion): # send a certain proportion of images for training, and the rest for testing\n",
    "\n",
    "                training_features.append(process_file(url))\n",
    "                training_labels.append(get_label(url))\n",
    "\n",
    "            else: \n",
    "                testing_features.append(process_file(url))\n",
    "                testing_labels.append(get_label(url))\n",
    "                \n",
    "                \n",
    "    # these lists need to be converted to numpy arrays so the data conversion works, but essentially we're \n",
    "    # just returning the training_features, training_labels, testing_features, and testing_labels\n",
    "    # we've added to above. They hold the same content. \n",
    "    return np.asarray(training_features).astype('float32'), np.asarray(training_labels).astype('float32'), \n",
    "           np.asarray(testing_features).astype('float32'), np.asarray(testing_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36669425-6b8d-4fbf-b8ed-3ecf26eda6f0",
   "metadata": {},
   "source": [
    "Let's use this function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c460d62-0641-4a2b-8e3d-6be1a4c97c2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T05:06:32.731099Z",
     "iopub.status.busy": "2022-12-31T05:06:32.730917Z",
     "iopub.status.idle": "2022-12-31T05:06:34.114441Z",
     "shell.execute_reply": "2022-12-31T05:06:34.113551Z",
     "shell.execute_reply.started": "2022-12-31T05:06:32.731083Z"
    }
   },
   "outputs": [],
   "source": [
    "training_features, training_labels, testing_features, testing_labels = create_dataset([cat_images, dog_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a03a298-bce8-4954-aa13-90e794f0eb45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T05:27:23.766866Z",
     "iopub.status.busy": "2022-12-31T05:27:23.766551Z",
     "iopub.status.idle": "2022-12-31T05:27:24.387924Z",
     "shell.execute_reply": "2022-12-31T05:27:24.387305Z",
     "shell.execute_reply.started": "2022-12-31T05:27:23.766845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 05:27:24.180430: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 96337920 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((training_features, training_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((testing_features, testing_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f42e9-0cd5-4236-bc1b-17c6f9d7da10",
   "metadata": {},
   "source": [
    "4. Create the model\n",
    "\n",
    "Now that we have our dataset created, it's time to create our model so we can train it! We're going to be using a convolutional \n",
    "neural network to analyze our images. In the previous chapter, we used a multilayered perceptron model. The issue with that\n",
    "is it can only handle 1 dimensional data... but our images aren't 1 dimensional. The same trick we used last time (flattening the\n",
    "images) won't work this time because these images are much more complex. Instead, we need our model to be able to actually\n",
    "use 2D images without destroying spatial continuity (how close pixels are together, this is destroyed when flattening images). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92934a85-98ca-4870-a302-f2b130dc258b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T05:36:25.610582Z",
     "iopub.status.busy": "2022-12-31T05:36:25.609816Z",
     "iopub.status.idle": "2022-12-31T05:41:17.378091Z",
     "shell.execute_reply": "2022-12-31T05:41:17.376976Z",
     "shell.execute_reply.started": "2022-12-31T05:36:25.610553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 30s 3s/step - loss: 15.9645 - accuracy: 0.5000 - val_loss: 1.6429 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 1.3036 - accuracy: 0.2000 - val_loss: 1.0915 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 1.0491 - accuracy: 0.2000 - val_loss: 0.7175 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7417 - accuracy: 0.1688 - val_loss: 0.6927 - val_accuracy: 0.6000\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6941 - accuracy: 0.3750 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6927 - accuracy: 0.5250 - val_loss: 0.6935 - val_accuracy: 0.4500\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6914 - accuracy: 0.5688 - val_loss: 0.6940 - val_accuracy: 0.4750\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7475 - accuracy: 0.5375 - val_loss: 0.6938 - val_accuracy: 0.5500\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(2))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset.batch(16), epochs=10, \n",
    "                    validation_data=test_dataset.batch(16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
