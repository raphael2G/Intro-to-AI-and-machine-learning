{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download Dependencies\n",
    "%pip install -q jmd_imagescraper\n",
    "%pip install tensorflow_datasets==4.7\n",
    "%pip install os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the imports below does not work, you may need to restart the kernel. This can be done in top right \"RESTART KERNEL\" button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Import Dependencies\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import Sequential, losses, metrics, layers\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from jmd_imagescraper.core import * # duckduckgo_search comes from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root = Path().cwd()/\"images\"\n",
    "\n",
    "cat_images = duckduckgo_search(root, \"Cats\", \"cute kittens\", max_results=100)\n",
    "dog_images = duckduckgo_search(root, \"Dogs\", \"cute puppies\", max_results=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['Cats', 'Dogs']\n",
    "\n",
    "def get_label(file_path):\n",
    "    label = os.path.normpath(file_path).split(os.path.sep)[-2] # extract the class from the label\n",
    "    return CLASSES.index(label) # return the label \n",
    "\n",
    "def process_file(file_path, img_size=224):\n",
    "    img = Image.open(file_path) # load the image from the url\n",
    "    img = img.resize((img_size, img_size), Image.Resampling.BILINEAR) # resize the image to 224 x 224 pixels\n",
    "    img = np.asarray(img) # convert Image to np array\n",
    "    img = img/255.0 # scale image between 0 and 1 to improve model performance\n",
    "    return img # return the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(datasets, training_proportion=0.8): # takes in list of image_urls ex. [cat_images, dog_images]\n",
    "    \n",
    "    training_features, training_labels = [], []\n",
    "    testing_features, testing_labels = [], []\n",
    "    \n",
    "    for image_urls in datasets: # extract image_urls of a single class ex. cat_images\n",
    "        for index, url in enumerate(image_urls): # loop through every url in cat_images\n",
    "\n",
    "            if index < (len(image_urls) * training_proportion): # send a certain proportion of images for training, and the rest for testing\n",
    "\n",
    "                training_features.append(process_file(url))\n",
    "                training_labels.append(get_label(url))\n",
    "\n",
    "            else: \n",
    "                testing_features.append(process_file(url))\n",
    "                testing_labels.append(get_label(url))\n",
    "                \n",
    "    return training_features, training_labels, testing_features, testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features, training_labels, testing_features, testing_labels = create_dataset([cat_images, dog_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_np = np.asarray(training_labels).astype('float32')\n",
    "training_features_np = np.asarray(training_features).astype('float32')\n",
    "\n",
    "testing_labels_np = np.asarray(testing_labels).astype('float32')\n",
    "testing_features_np = np.asarray(testing_features).astype('float32')\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((training_features_np, training_labels_np))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((testing_features_np, testing_labels_np))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(2))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset.batch(16), epochs=10, \n",
    "                    validation_data=test_dataset.batch(16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf8ec2b10e5872c21395741e4882c7bbe6c5767097b3ea8dc8629f71b15614b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
